from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
import time
import math
import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
plt.switch_backend('agg')
import matplotlib.ticker as ticker
import numpy as np
from os import system
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
import os
from tensorboardX import SummaryWriter



"""========================================================================================
The sample.py includes the following template functions:

1. Encoder, decoder
2. Training function
3. BLEU-4 score function
4. Gaussian score function

You have to modify them to complete the lab.
In addition, there are still other functions that you have to 
implement by yourself.

1. The reparameterization trick
2. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)
3. Output your results (BLEU-4 score, conversion words, Gaussian score, generation words)
4. Plot loss/score
5. Load/save weights

There are some useful tips listed in the lab assignment.
You should check them before starting your lab.
========================================================================================"""

folder_name = 'cyclical_new_t1_e40_lr0.01_decay10e-4_ce1/'

params_save_dir = './experiments/'+folder_name
log_dir = './logs/'+folder_name
if not os.path.isdir(params_save_dir):
    os.makedirs(params_save_dir)
if not os.path.isdir(log_dir):
    os.makedirs(log_dir)

cuda = True if torch.cuda.is_available() else False
GPUID = [0]
torch.cuda.set_device(GPUID[0])
SOS_token = 0
EOS_token = 1
#----------Hyper Parameters----------#
hidden_size = 256
#The number of vocabulary
vocab_size = 28
teacher_forcing_ratio = 1
empty_input_ratio = 0.1
KLD_weight = 0.0
LR = 0.01
MAX_LENGTH = 20
is_train = True

### 0:simple present(sp), 1:third person(tp), 2:present progressive(pg), 3:simple past(p).

################################
#Example inputs of compute_bleu
################################
#The target word
reference = 'accessed'
#The word generated by your model
output = 'access'

#compute BLEU-4 score
def compute_bleu(output, reference):
    cc = SmoothingFunction()
    if len(reference) == 3:
        weights = (0.33,0.33,0.33)
    else:
        weights = (0.25,0.25,0.25,0.25)
    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)


"""============================================================================
example input of Gaussian_score

words = [['consult', 'consults', 'consulting', 'consulted'],
['plead', 'pleads', 'pleading', 'pleaded'],
['explain', 'explains', 'explaining', 'explained'],
['amuse', 'amuses', 'amusing', 'amused'], ....]

the order should be : simple present, third person, present progressive, past
============================================================================"""

def Gaussian_score(words):
    words_list = []
    score = 0
    yourpath = './dataset/train.txt'#should be your directory of train.txt
    with open(yourpath,'r') as fp:
        for line in fp:
            word = line.split(' ')
            word[3] = word[3].strip('\n')
            words_list.extend([word])
        # print(words_list)
        for t in words:
            for i in words_list:
                if t == i:
                    score += 1
                    # print("i, t", i, t)
    return score/len(words)


def get_z_encode(mu, logvar):
    std = logvar.mul(0.5).exp_()
    eps = torch.randn(std.size(0), std.size(1))
    z = eps.mul(std).add_(mu)
    return z

#Encoder
class EncoderRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(EncoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding_word = nn.Embedding(input_size, hidden_size)
        self.embedding_tense = nn.Embedding(input_size, int(hidden_size/2))

        self.lstm = nn.LSTM(hidden_size, hidden_size)
        self.fc = nn.Linear(hidden_size,int(hidden_size/2))
        self.fcVar = nn.Linear(hidden_size,int(hidden_size/2))

    def forward(self, input, hidden, c_state, last=False):
        if last:
            embedded = self.embedding_word(input).view(1, 1, -1)
            output = embedded
            output, (hidden, c_state) = self.lstm(output, (hidden, c_state))
            mu = self.fc(hidden)
            logvar = self.fcVar(hidden)
            return mu, logvar

        embedded = self.embedding_word(input).view(1, 1, -1)
        output = embedded
        output, (hidden, c_state) = self.lstm(output, (hidden, c_state))
        return output, hidden, c_state

    def initHidden(self, tense):
        cond = self.embedding_tense(tense).view(1, 1, -1)

        return torch.cat((torch.zeros(1, 1, int(self.hidden_size/2)).cuda(GPUID[0]), cond), 2)


#Decoder
class DecoderRNN(nn.Module):
    def __init__(self, hidden_size, output_size):
        super(DecoderRNN, self).__init__()
        self.hidden_size = hidden_size

        self.embedding_word = nn.Embedding(output_size, hidden_size)
        self.embedding_tense = nn.Embedding(4, int(hidden_size/2))

        self.lstm = nn.LSTM(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden, c_state):
        output = self.embedding_word(input).view(1, 1, -1)
        output = F.relu(output)
        output, (hidden, c_state) = self.lstm(output, (hidden, c_state))
        output = self.out(output[0])
        return output, hidden, c_state

    def initHidden(self, z, tense):
        cond = self.embedding_tense(tense).view(1, 1, -1)

        return torch.cat((z.cuda(GPUID[0]), cond), 2)


def train(input_tensor, target_tensor, input_tense, target_tense, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, KLD_weight, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden(input_tense.cuda(GPUID[0]))
    encoder_c_state = encoder_hidden
  
    encoder_optimizer.zero_grad()
    decoder_optimizer.zero_grad()

    input_length = input_tensor.size(0)
    target_length = target_tensor.size(0)

    cross_entropy_loss = 0

    #----------sequence to sequence part for encoder----------#
    for ei in range(input_length):
        if input_tensor[ei] == 1:
            mu, logvar = encoder(input_tensor[ei].cuda(GPUID[0]), encoder_hidden.cuda(GPUID[0]), encoder_c_state.cuda(GPUID[0]), True)
        else:
            encoder_output, encoder_hidden, encoder_c_state = encoder(input_tensor[ei].cuda(GPUID[0]), encoder_hidden.cuda(GPUID[0]), encoder_c_state.cuda(GPUID[0]))
    
    z = get_z_encode(mu.cpu(), logvar.cpu())

    decoder_input = torch.tensor([[SOS_token]])
    decoder_hidden = decoder.initHidden(z.cuda(GPUID[0]), target_tense.cuda(GPUID[0]))
    decoder_c_state = decoder_hidden

    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False
	
    #----------sequence to sequence part for decoder----------#
    if use_teacher_forcing:
        # Teacher forcing: Feed the target as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_c_state = decoder(
                decoder_input.cuda(GPUID[0]), decoder_hidden.cuda(GPUID[0]), decoder_c_state.cuda(GPUID[0]))
            cross_entropy_loss += criterion(decoder_output, target_tensor[di])
            decoder_input = target_tensor[di]  # Teacher forcing

    else:
        # Without teacher forcing: use its own predictions as the next input
        for di in range(target_length):
            decoder_output, decoder_hidden, decoder_c_state = decoder(
                decoder_input.cuda(GPUID[0]), decoder_hidden.cuda(GPUID[0]), decoder_c_state.cuda(GPUID[0]))
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            cross_entropy_loss += criterion(decoder_output, target_tensor[di])
            if decoder_input.item() == EOS_token:
                break

    z_kl_loss = -0.5 * torch.sum(1 + logvar.cuda(GPUID[0]) - mu.cuda(GPUID[0]).pow(2) - logvar.cuda(GPUID[0]).exp())*KLD_weight
    ce_weight = 1

    loss = z_kl_loss + cross_entropy_loss*ce_weight
)
    loss.backward()

    encoder_optimizer.step()
    decoder_optimizer.step()

    return cross_entropy_loss.item() / target_length, cross_entropy_loss*ce_weight, z_kl_loss, loss


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))


def str_to_int(x_str):
    return np.array([ord(x)-ord('a')+2 for x in x_str])

def int_to_str(x_int):
    return "".join(np.array([chr(x + ord('a')-2) for x in x_int]))


def build_pairs(data, tense_list, index):
    for i in index:
        data.append((tense_list[i[0]], tense_list[i[1]], torch.tensor(i[0]), torch.tensor(i[1])))
    return data



def dataloader(mode):
    if mode == 'train':
        data_word = []
        index = np.delete(np.array(np.meshgrid([0, 1, 2, 3], [0, 1, 2, 3])).T.reshape(-1,2), [0, 5, 10, 15], axis=0)

        with open('./dataset/train.txt') as f:
            for line in f.readlines():
                word_list = line.strip().split(' ')
                tense_list = []
                for word in word_list:
                    word_input = torch.from_numpy(np.append(str_to_int(word), EOS_token)).view(-1, 1)
                    # word_input = word
                    tense_list.append(word_input)
                
                data_word = build_pairs(data_word, tense_list, index)

        return data_word
    else:
        with open('./dataset/test.txt') as f:
            data_word = []
            origin = []
            for line in f.readlines():
                word_list = line.strip().split(' ')
                word1 = word_list[0]
                word2 = word_list[1]
                tense1 = int(word_list[2])
                tense2 = int(word_list[3])
                origin.append((word1, word2, tense1, tense2))
                word1_tenser = torch.from_numpy(np.append(str_to_int(word1), EOS_token)).view(-1, 1)
                word2_tenser = torch.from_numpy(np.append(str_to_int(word2), EOS_token)).view(-1, 1)
                data_word.append((word1_tenser, word2_tenser, torch.tensor(tense1), torch.tensor(tense2)))

        return data_word, origin


def trainIters(encoder, decoder, pairs, n_iters, kl_mode, print_every=1000, plot_every=100, learning_rate=0.01, KLD_weight=0):
    start = time.time()
    plot_losses = []
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every
    ce_loss_avg = 0
    kl_loss_avg = 0
    total_loss_avg = 0
    encoder.train()
    decoder.train()

    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate,  weight_decay=1e-4)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate,  weight_decay=1e-4)

    training_pairs = [random.choice(pairs)
                      for i in range(n_iters)]

    criterion = nn.CrossEntropyLoss()
    
    kl_count = 0

    for iter in range(1, n_iters + 1):
        training_pair = training_pairs[iter - 1]
        input_tensor = training_pair[0]
        target_tensor = training_pair[1]
        input_tense = training_pair[2]
        target_tense = training_pair[3]

        loss, ce_loss, kl_loss, total_loss = train(input_tensor.cuda(GPUID[0]), target_tensor.cuda(GPUID[0]), input_tense.cuda(GPUID[0]), target_tense.cuda(GPUID[0]), encoder.cuda(GPUID[0]),
                     decoder, encoder_optimizer, decoder_optimizer, criterion, KLD_weight)



        print_loss_total += loss
        plot_loss_total += loss
        ce_loss_avg += ce_loss
        kl_loss_avg += kl_loss
        total_loss_avg += total_loss
        
        if iter % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),
                                         iter, iter / n_iters * 100, print_loss_avg))
        
        if kl_mode == 'c':
            KLD_decay = 2
            if kl_count < KLD_decay:
                KLD_weight += (1/KLD_decay)
                kl_count += 1
            elif kl_count == KLD_decay*2:
                KLD_weight = 0
                kl_count = 0
            else:
                KLD_weight = 1
                kl_count += 1
        elif kl_mode == 'm':
            KLD_decay = 75000*1

            if epoch*iter < KLD_decay:
                KLD_weight += (1/KLD_decay)
            else:
                KLD_weight = 1
            
        else:
            print("wrong mode")
            exit(1)
        

    plot_loss_total /= n_iters
    ce_loss_avg /= n_iters
    kl_loss_avg /= n_iters
    total_loss_avg /= n_iters


    writer.add_scalar('/loss/total_loss_avg', total_loss_avg, epoch)
    writer.add_scalar('/loss/kl_loss_avg', kl_loss_avg, epoch)
    writer.add_scalar('/loss/ce_loss_avg', ce_loss_avg, epoch)
    writer.add_scalar('/loss/loss_avg', plot_loss_total, epoch)
    torch.save(encoder.state_dict(), '%sencoder_%d.pth'%(params_save_dir, epoch))
    torch.save(decoder.state_dict(), '%sdecoder_%d.pth'%(params_save_dir, epoch))


def test(input_tensor, input_tense, target_tense, encoder, decoder, max_length=MAX_LENGTH):
    encoder_hidden = encoder.initHidden(input_tense.cuda(GPUID[0]))
    encoder_c_state = encoder_hidden

    input_length = input_tensor.size(0)


    #----------sequence to sequence part for encoder----------#
    for ei in range(input_length):
        if input_tensor[ei] == 1:
            # print(input_tensor[ei])
            mu, logvar = encoder(input_tensor[ei].cuda(GPUID[0]), encoder_hidden.cuda(GPUID[0]), encoder_c_state.cuda(GPUID[0]), True)
        else:
            encoder_output, encoder_hidden, encoder_c_state = encoder(input_tensor[ei].cuda(GPUID[0]), encoder_hidden.cuda(GPUID[0]), \
                                                     encoder_c_state.cuda(GPUID[0]))
    
    z = get_z_encode(mu.cpu(), logvar.cpu())

    decoder_input = torch.tensor([[SOS_token]])
    decoder_hidden = decoder.initHidden(z.cuda(GPUID[0]), target_tense.cuda(GPUID[0]))
    decoder_c_state = decoder_hidden

    #----------sequence to sequence part for decoder----------#

    pred = []
    # Without teacher forcing: use its own predictions as the next input
    for di in range(max_length):
        decoder_output, decoder_hidden, decoder_c_state = decoder(
            decoder_input.cuda(GPUID[0]), decoder_hidden.cuda(GPUID[0]), decoder_c_state.cuda(GPUID[0]))
        topv, topi = decoder_output.topk(1)
        decoder_input = topi.squeeze().detach()  # detach from history as input

        if decoder_input.item() == EOS_token:
            break
        pred.append(decoder_input.cpu().numpy())

    return int_to_str(pred)


def testIters(encoder, decoder, n_iters, testing_pairs, origin_txt):
    start = time.time()
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder.eval()
    decoder.eval()

    bleu = 0

    with torch.no_grad():
        for iter in range(1, n_iters + 1):

            testing_pair = testing_pairs[iter - 1]
            input_tensor = testing_pair[0]
            target_tensor = testing_pair[1]
            input_tense = testing_pair[2]
            target_tense = testing_pair[3]


            predict = test(input_tensor.cuda(GPUID[0]), input_tense.cuda(GPUID[0]), target_tense.cuda(GPUID[0]), encoder.cuda(GPUID[0]), decoder.cuda(GPUID[0]))
            bleu += compute_bleu(origin_txt[iter - 1][1], predict)

        bleu /= n_iters
        print("%d BLEU-4 score:%f" % (epoch, bleu))
        writer.add_scalar('/score/BLEU-4', bleu, epoch)

        words = []
        for iter in range(100):
            pred_list = word_generation(encoder, decoder)
            words.append(pred_list)
        g_score = Gaussian_score(words)
        print(g_score)
        writer.add_scalar('/score/Gaussian', g_score, epoch)

def word_generation(encoder, decoder, max_length=MAX_LENGTH):
    z = torch.randn(1, 1, 128)
    pred_list = []
    for tense in range(4):
        target_tense = torch.tensor(tense)

        decoder_input = torch.tensor([[SOS_token]])
        decoder_hidden = decoder.initHidden(z.cuda(GPUID[0]), target_tense.cuda(GPUID[0]))
        decoder_c_state = decoder_hidden
     
        #----------sequence to sequence part for decoder----------#

        pred = []
        # Without teacher forcing: use its own predictions as the next input
        for di in range(max_length):
            decoder_output, decoder_hidden, decoder_c_state = decoder(
                decoder_input.cuda(GPUID[0]), decoder_hidden.cuda(GPUID[0]), decoder_c_state.cuda(GPUID[0]))
            topv, topi = decoder_output.topk(1)
            decoder_input = topi.squeeze().detach()  # detach from history as input

            if decoder_input.item() == EOS_token:
                break
            pred.append(decoder_input.cpu().numpy())
        pred_list.append(int_to_str(pred))

    return pred_list



def testFinal(encoder, decoder, load_model, n_iters, testing_pairs, origin_txt):
    start = time.time()
    print_loss_total = 0  # Reset every print_every
    plot_loss_total = 0  # Reset every plot_every

    encoder.load_state_dict(torch.load('%sencoder_%d.pth'%(params_save_dir, load_model)))
    decoder.load_state_dict(torch.load('%sdecoder_%d.pth'%(params_save_dir, load_model)))
    encoder.eval()
    decoder.eval()
    
    bleu = 0

    with torch.no_grad():
        for iter in range(1, n_iters + 1):
            # print(iter)
            testing_pair = testing_pairs[iter - 1]
            input_tensor = testing_pair[0]
            target_tensor = testing_pair[1]
            input_tense = testing_pair[2]
            target_tense = testing_pair[3]

            predict = test(input_tensor.cuda(GPUID[0]), input_tense.cuda(GPUID[0]), target_tense.cuda(GPUID[0]), encoder.cuda(GPUID[0]), decoder.cuda(GPUID[0]))
            
            print("==========================================")

            print("input:%s" % (origin_txt[iter - 1][0]))
            print("target:%s" % (origin_txt[iter - 1][1]))
            print("pred:%s" % (predict))
            bleu += compute_bleu(origin_txt[iter - 1][1], predict)
        bleu /= n_iters
        print("BLEU-4 score:%f" % (bleu))

        words = []
        for iter in range(100):
            pred_list = word_generation(encoder, decoder)
            words.append(pred_list)
            print(pred_list)
        # words = [['designate','designates','designating','designated']]
        # print(words)
        print("Gaussian score:", Gaussian_score(words))
            
            


encoder1 = EncoderRNN(vocab_size, hidden_size)
decoder1 = DecoderRNN(hidden_size, vocab_size)
encoder1.cuda(GPUID[0])
decoder1.cuda(GPUID[0])
testing_pairs, origin_txt = dataloader('test')


if is_train:
    epoch_num = 40
    writer = SummaryWriter(log_dir)
    pairs = dataloader('train')
    for epoch in range(epoch_num):
        trainIters(encoder1, decoder1, pairs, 75000, folder_name[0], print_every=5000)
        testIters(encoder1.cuda(GPUID[0]), decoder1.cuda(GPUID[0]), 10, testing_pairs, origin_txt)
else:
    testFinal(encoder1.cuda(GPUID[0]), decoder1.cuda(GPUID[0]), 32, 10, testing_pairs, origin_txt)

